[

	{
        "title" : "Garbage Collecting Robot",
        "categories" : ["robotics", "machine learning", "embedded systems"],
        "date": "2013 November",
        "post_body" : "Having learned from the previous year's mistakes and triumphs, our team was back at UBC's annual engineering competition.  This year, we were asked to build a robot to mimic an autonomous garbage truck whose responsibility was to guide itself from a starting location to a garbage can, pick up the garbage can, and proceed to tour a row of homes to pick up trash.<br>     Like last year, all teams were provided with the VEX robotic kit, which was a kit consisting of many mechanical components, sensors (ultrasonic, IR, etc), and a microcontroller with a C compiler.  All locations that needed to be traveled to were connected together with black tape, and there were a few misleading trails of black tape and numerous ramps placed along the track to test our robot's software complexity and mechanical capabilites. <br><br>  When we entered the previous year's competition, our robot only had two light sensors mounted.  The light sensors were mounted facing the ground, and were arguably the most crucial sensor in our system since they kept track of the black tape and ensured our robot doesn't steer off course.  The problem with two sensors was the massive amount of zigzagging done by the robot as it tried to correct it's course, since the correction didn't occur until the robot was very crooked. This year, we added a third light sensor in the center, which heavily reduced the zigzagging of our robot.  An ultrasonic sensor was placed at the front of the vehicle to detect the garbage.  When the garbage was close enough, our robot would use a mechanical arm to sweep down and place the garbage inside the garbage can.  <br><br><center><img src='json/projects/images/sec2013/IMG_8824.JPG' style='width:304px;height:228px'><br><i>Aerial view of robot.  Three light sensors face the bottom and ultrasonic sensor faces the front</i></center><br>      <br><center><img src='json/projects/images/sec2013/IMG_8827.JPG' style='width:304px;height:228px'><br><i>Side of the robot.  Servos were used to control back wheel.</i></center><br>    Due to the 5 hour limitation of the competition, we were unable to complete our robot's mechanical garbage arm.  However, most teams competing were in a similar situation, and we were asked to demo our robot on portions of the obstacle course it could complete.  The judges loved the design principles our team used to build this robot, especially the rapid prototyping process we used to finetune the path adjustment algorithm using our three light sensors.  Ultimately, our team came just short, and we finished 2nd place in the competition.    <br><br><center><img src='json/projects/images/sec2013/IMG_8832.JPG' style='width:304px;height:228px'><br><i>Cracking a smile(x2) after our presentation!</i></center>    <br><br><center><img src='json/projects/images/sec2013/IMG_8823.JPG' style='width:304px;height:228px'><br><i>2nd place!  Going for the GOLD next year!</i></center>"
    },

    {
        "title" : "Autonomous Firetruck",
        "categories" : ["robotics", "machine learning", "embedded systems"],
        "date": "2012 October",
        "post_body" : "    As part of the University of British Columbia's schoolwide engineering competition in 2012, all competitors were asked to build an autonomous robot that could travel from a starting location, pick up a can of water from a second location, put out a fire at the third location, and return to the starting location. The path connecting all these locations were marked by black tape and the terrain varied throughout the course, with several obstacles, ramps, and gaps that our robot had to avoid.<br><br><center><img src='json/projects/images/sec2012/IMG_6440.JPG' style='width:304px;height:228px'><br><i>Closely examining the course terrain prior to competition</i></center><br>    To build the robot, all teams were provided with the VEX robotic kit, consisting of a large number of mechanical components, sensors, and a microcontroller to control the robot autonomously. <br><br>    The most essential problem was traveling through the course by following the black line connecting all the locations.  To accomplish this, one light sensor was placed on each side of our robot.  If our robot were to travel off the track, the light sensor will eventually sense the black line - This indicates our robot has travelled too far to the opposite side of the triggered light sensor.  To account for this issue, we simply turn our robot in either a clockwise or counterclockwise direction (depending on which side the light sensor was triggered), and move forward again.  This process is repeated until both light sensors are eventually untriggered, indicating that our robot is traveling in a straight line along the intended path.  <br><br><center><img src='json/projects/images/sec2012/IMG_6457.JPG' style='width:304px;height:228px';><br><i>Back of the robot.  Blue rectangular box is the VEX microcontroller and red box on the side is the light sensor</i></center><br><br><center><img src='json/projects/images/sec2012/IMG_6459.JPG' style='width:304px;height:228px'><br><i>Front of the robot.  Red box with with black spots is the ultrasonic sensor</i></center><br> To pick up the water bottle, our team decided to use a ultrasonic sensor at the front of the robot to find the bottle of water.  Once the waterbottle was within range of our sensor, the mechanical arm on our robot scooped the bottle of water up.  Lastly, we travel to the destination and lower the mechanical arm to lower the bottle of water into the hypothetical fire, effectively putting it out.  Our robot then turns around and returns to it's starting location.  <br><br>    To grade competitors, judges ranked all robots based on principles of design, functionality, and oral presentation.  Taking all three areas into account, my team placed 3rd in the competition.        <br><br><center><img src='json/projects/images/sec2012/IMG_6464.JPG' style='width:304px;height:228px'><br><i>Presenting...  Your UBC Engineering Competition Second Runnerups!</i></center>"
    },

	{
		"title": "Electro-Mechanical LED Display"
		"categories": ["signal generator", "mechanical", "digital logic"]
		"date": "2011 November"
		"post_body": "For my second year project course, my team and I built a electro-mechanical LED display that takes a wave produced by a signal generator as input and outputs the exact same wave onto a row of LEDs.  The device is designed to display the waveform produced by the function generator by rapidly moving a column of 16 LEDs back and forth in a linear fashion using a mechanical arm powered by a DC motor.  The device must operate at a constant speed that is fast enough so the human eye can perceive the waveform.  <br><br>  The Scotch-Yoke mechanism was chosen to guide our mechanical arm since it is more efficient in converting circular motion produced by the motor into linear movement of the cart along the rail.  It consists of a pin attached to the wheel that moves down a slot, pushing the form to move in a straight line.  <br><br>  The logic was implemented using a analog-to-digital converter.  The analog signal ranged from 5V to -5V, and our ADC translated this value into an 8 bit binary figure.  An analog voltage value of 5V (highest) would translate to 255 (1111 1111 in binary) digital output, while a -5V input voltage would translate to a 0 (0000 0000 in binary) digital output.  This digital output value is then processed with an Altera FPGA board, mapping these into a row of 16 LEDs.  Depending on the input value at the time, a different LED is lit up, and this calculation is repeated many times in parallel with the movement of the mechanical arm, to make the LEDs display an identical wave to what is produced by the signal generator.  <br><br>  As an extra feature, our team built the capability to vary the voltage per division and also the time per division of the waveform displayed.  By pressing push buttons on our Altera FPGA board, we could control the scale in both the X and Y axis that were displayed by our LEDs."
	}
]

